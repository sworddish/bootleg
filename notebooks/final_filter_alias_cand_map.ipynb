{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootleg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print (os.environ['CONDA_DEFAULT_ENV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import ujson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load alias map to filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bootleg.symbols.entity_profile import EntityProfile\n",
    "root_dir = Path(\"/opt/data/cchang/bootleg_train/data\")\n",
    "entity_dump = EntityProfile.load_from_cache(load_dir=root_dir / \"entity_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_aliases = entity_dump._entity_symbols.get_alias2qids_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load count files for all of wikipedia --- these were computed with `compute_statistics.py` (in utils/preprocessing) over the merged data file of test, dev, and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of times alias phrase occurs in the text across ALL of wikipedia\n",
    "alias_text_counts = ujson.load(\n",
    "    open(root_dir / 'stats/alias_text_counts.json'))\n",
    "\n",
    "# number of times alias occurs as an alias across ALL of wikipedia\n",
    "alias_counts = ujson.load(\n",
    "    open(root_dir / 'stats/alias_counts.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple function to find aliases to remove based on the count files above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_value(alias, verbose=False):\n",
    "    if verbose:\n",
    "        print('# times occurs as alias:', alias_counts.get(alias, 0))\n",
    "        print('# times occurs in text:', alias_text_counts.get(alias, 0))\n",
    "    return alias_counts.get(alias, 0) / (alias_text_counts[alias]) if alias in alias_text_counts else -1\n",
    "\n",
    "\n",
    "def get_aliases_to_remove(curr_aliases, keep_wikidata=False, norm_threshold=0.017, min_seen=500, min_alias_count=10000):\n",
    "    \"\"\"\n",
    "    Remove aliases which are frequent words but infrequent aliases due to rarity \n",
    "    or mislabel (e.g. band \"themselves\").\n",
    "    \"\"\"\n",
    "    aliases_to_remove = set()\n",
    "    cnts = defaultdict(int)\n",
    "    grps = defaultdict(list)\n",
    "    for alias in tqdm(curr_aliases):\n",
    "        # If alias is not seen in Wikipedia\n",
    "        if alias not in alias_counts:\n",
    "            # If alias is seen in text but only a few times, skip as it's too few to make a decision\n",
    "            if (alias in alias_text_counts and alias_text_counts[alias] < min_seen):\n",
    "                continue\n",
    "            # if alias occurs in Wikidata (so it's in our alias map), but not as alias in Wikipedia\n",
    "            # and occurs more than min_seen times, only keep if one candidate (indicating a fairly unique alias)\n",
    "            # and if that one candidate is a type we care about (e.g., people and locations)\n",
    "            elif len(curr_aliases[alias]) == 1:\n",
    "                continue\n",
    "            # else make sure we don't think it's a person or location name - we want to keep those\n",
    "            # even if more general alias\n",
    "            else:\n",
    "                if keep_wikidata:\n",
    "                    continue\n",
    "                cnts[\"not_in_wikipedia\"] += 1\n",
    "                grps[\"not_in_wikipedia\"].append(alias)\n",
    "                aliases_to_remove.add(alias)\n",
    "                continue \n",
    "        # length greater than max_alias_len and weak labels cause some aliases to occur as aliases \n",
    "        # but not occur in the text\n",
    "        if alias not in alias_text_counts:\n",
    "            continue \n",
    "        # filter out aliases which occur commonly in the text but uncommonly as an alias\n",
    "        # we require that the alias is a common phrase in text \n",
    "        # and that the phrase isn't very commonly an alias \n",
    "        if (get_norm_value(alias) < norm_threshold):\n",
    "            if alias_text_counts[alias] > min_seen:\n",
    "                if alias_counts[alias] < min_alias_count:\n",
    "                    aliases_to_remove.add(alias)\n",
    "                    cnts[\"removed_filter\"] += 1\n",
    "                    grps[\"removed_filter\"].append(alias)\n",
    "                else:\n",
    "                    cnts[\"grt_min_alias_cnt\"] += 1\n",
    "                    grps[\"grt_min_alias_cnt\"].append(alias)\n",
    "            else:\n",
    "                cnts[\"lt_min_seen\"] += 1\n",
    "                grps[\"lt_min_seen\"].append(alias)\n",
    "    \n",
    "    return aliases_to_remove, cnts, grps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 2326245/2326245 [00:01<00:00, 1405098.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"not_in_wikipedia\": 24852,\n",
      "    \"lt_min_seen\": 26,\n",
      "    \"removed_filter\": 14\n",
      "}\n",
      "Will remove 24866 out of 2326245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "aliases_to_remove, cnts, grps = get_aliases_to_remove(curr_aliases)\n",
    "print(ujson.dumps(cnts, indent=4))\n",
    "print(f\"Will remove {len(aliases_to_remove)} out of {len(curr_aliases)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity checks on the filter step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "三墩站\n",
      "下多布林卡农村居民点\n",
      "巴尔卡尼乡\n",
      "岭南四大园林\n",
      "王氏大宗祠\n",
      "七十三\n",
      "善夫\n",
      "墨西哥卷\n",
      "雷涛\n",
      "第25届fism世界魔术大会\n",
      "新化站\n",
      "阿史那 (突厥)\n",
      "乌戈尔语\n",
      "六十\n",
      "加拉西米夫卡\n",
      "圣克雷阿克\n",
      "vaio fs\n",
      "微毛诃子\n",
      "葛麻姆\n",
      "乐民河\n",
      "黑皮油松\n",
      "哥姆巴尼甘杰乌帕齐拉\n",
      "aich\n",
      "煤炭龟\n",
      "沃夫昌西克\n",
      "帕拉图\n",
      "美国领导的对叙利亚的军事干预\n",
      "刺齿复叶耳蕨\n",
      "超铁暴龙\n",
      "解放军东北军需学校\n",
      "印度驻中华民国大使列表\n",
      "与立\n",
      "舞鹤东交流道\n",
      "mondi\n",
      "psp slim lite\n",
      "朱家相\n",
      "gba机模拟器\n",
      "nokia internet tablet\n",
      "新达尼利夫卡\n",
      "欲海奇女子\n",
      "烫衣机\n",
      "詹姆斯·金\n",
      "马蹄决明\n",
      "罗吉兹纳\n",
      "双结节神螺\n",
      "马徐妍\n",
      "蒙莫尔\n",
      "中国主席\n",
      "布兰维尔\n",
      "库佩瓦哈\n"
     ]
    }
   ],
   "source": [
    "# sample what aliases are getting removed\n",
    "num_to_sample = 50\n",
    "for alias in np.random.choice(list(aliases_to_remove), num_to_sample): \n",
    "    print(alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for existence of certain words in aliases_to_remove\n",
    "sanity_checks = [('the', True), \n",
    "                 ('你好', False),\n",
    "                 ('七十三', True),\n",
    "                 ('我', False)\n",
    "                ]\n",
    "for s, bool_val in sanity_checks: \n",
    "    assert (s in aliases_to_remove) is bool_val, f'{s} {bool_val} {s in aliases_to_remove}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove aliases and save new candidate mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading edit mode, may take some minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading edit mode, may take some minutes\")\n",
    "entity_dump_edit = EntityProfile.load_from_cache(load_dir=root_dir / \"entity_db\", edit_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 24866/24866 [00:00<00:00, 74148.39it/s]\n"
     ]
    }
   ],
   "source": [
    "for alias in tqdm(aliases_to_remove):\n",
    "    for qid in list(entity_dump_edit.get_qid_cands(alias)):\n",
    "        entity_dump_edit.remove_mention(qid, alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197863b1e52d4bc98e62a00cb4b5cf03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating trie:   0%|          | 0/2301379 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 1.4773750868501017e-05% of items that lost information because max_connections was too small.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04af4dc14a0345ddbe5b124a009fba26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating trie:   0%|          | 0/1252201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 0.08978430779084189% of items that lost information because max_connections was too small.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662aa4fad8914c39b3be640c0f4eef75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Prepping trie data:   0%|          | 0/1252201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating trie with 1252201 values. This can take a few minutes.\n",
      "There were 0.000967097135364051% of items that lost information because max_connections was too small.\n"
     ]
    }
   ],
   "source": [
    "new_dir = root_dir / 'entity_db_filt'\n",
    "entity_dump_edit.save(new_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
